<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content>
    <meta name="author" content>

    <title>Clare Lyle</title>
    <link rel="icon" href="./images/c_icon.png" sizes="32x32" />
    <!-- Bootstrap Core CSS -->
    <link href="../css/bootstrap.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../css/clean-blog.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>
    <link rel="icon" href="./images/c_icon.png" sizes="32x32">
    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="../index.html">Clare Lyle</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="../index.html">Home</a>
                    </li>
                    <li>
                        <a href="../about.html">About</a>
                    </li>
                    <li>
                        <a href="../pubs.html">Publications</a>
                    </li>
                    <li>
                        <a href="../archive.html">Blog</a>
                    </li>
                    <li>
                        <a href="../contact.html">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('/images/robot_scenery_2.png'); box-shadow: inset 0 0 0 1000px rgba(0,0,0,.1); height:60px">
        <div class="container">
            <!-- <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"> -->
<!--                     <div class="site-heading">
                        <h1>Clare Lyle</h1> -->
                        <!--<hr class="small">
                        <span class="subheading">Machine Learning</span>-->
                    <!-- </div> -->
                <!-- </div>
            </div> -->
        </div>
    </header>

    <!-- Main Content -->
    <div class="container"><div class="info" style="font-family:Open Sans; margin-left: 5%">
    Posted on May 17, 2025
</div>
<div class="container">
<h2>A post-rigorous derivation of the Gauss-Newton matrix</h2>
<p><em>Why take a Hessian when a gradient will do?</em></p>
<p></p>
<p>Terry Tao has a wonderful blog post on the three phases of a mathematician’s career, dividing roughly into pre-, contemporaneous, and post-rigor. Having abandoned my pursuit of “real” mathematics at the age of 22, I can’t claim to have arrived at the enlightened post-rigor phase of my career through the honourable approach of a decade’s practical experience wielding proof techniques. Instead, having lost much of the patience I used to have for working out finnicky details of proofs and I’ve arrived at a kind of lazy post-rigor state which seems to be a good fit for blog posts.</p>
<p>We begin, as all stories about neural network optimization begin, with the gradient.</p>
<p><strong>Aside: what is the difference between a gradient and a derivative?</strong></p>
<p>This is a minor and pedantic point which almost doesn’t belong in a <em>post</em>-rigorous derivation of a second-order optimization methods. Nonetheless, I get the sense that a lot of machine learning people don’t know the difference between a derivative and a gradient, and I feel a moral obligation to clarify this.</p>
<p>If you remember your multivariable calculus, you’ll recall that a function <span class="math inline">\(f : \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> is <em>differentiable</em> if there’s some linear function <span class="math inline">\(\lambda : \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> such that</p>
<p><span class="math display">\[ \lim_{h \rightarrow 0} \frac{ | f(a + h) - f(a) - \lambda(h)|}{|h|} = 0 \]</span></p>
<p>for any <span class="math inline">\(h \in \mathbb{R}^n\)</span>. If such a <span class="math inline">\(\lambda\)</span> exists, then <span class="math inline">\(f\)</span> is differentiable with derivative <span class="math inline">\(\lambda\)</span>. Importantly, <span class="math inline">\(\lambda : \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> is a <em>function</em>, not a real number. When students are first introduced to calculus, they are usually taught that the derivative is the “rate of change” of a function. This is fine in 1-D because there’s only one direction that we can evaluate the linear function <span class="math inline">\(\lambda\)</span> on, so there’s a natural isomorphism between the derivative function <span class="math inline">\(\lambda\)</span> and its evaluation on a particular directional vector. For vector-valued functions, how fast and in what direction the function’s output is changing depends on the direction you are perturbing it in, so we need the derivative to be a function.</p>
<p>For a function <span class="math inline">\(f : \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, the derivative will be a function of the form <span class="math inline">\(\lambda(h) = \langle v, h \rangle\)</span> where <span class="math inline">\(v, h \in \mathbb{R}^n\)</span>, so we can identify the derivative with this vector <span class="math inline">\(v\)</span>. The <em>gradient</em> is this vector <span class="math inline">\(v\)</span>.
For a pedantic mathematician, this distinction is critical for gradient descent to be well-defined: you can’t add a function to a vector, but you <em>can</em> add two vectors together. So adding a scaled gradient to the current parameters to get an updated parameter set is totally kosher. In particular, when the function <span class="math inline">\(f\)</span> that we care about is a loss <span class="math inline">\(\ell\)</span>, and our vector <span class="math inline">\(v\)</span> is a set of parameters <span class="math inline">\(\theta\)</span> we recover the familiar update</p>
<p><span class="math display">\[ \theta \gets \theta - \eta \frac{\partial}{\partial \theta} \ell(\theta) \]</span></p>
<p><strong>Take two (derivatives)</strong></p>
<p>Gradient descent is the workhorse of neural network optimization. However, the standard “naive” gradient descent algorithm is typically only used in undergraduate projects and theory papers. Typically, some sort of rescaling is done to the gradients of a model before updates are applied to buffer against issues where the gradients are too large or too small to make meaningful changes to the parameters. For example, Sign SGD updates parameters based on the sign of the derivative of the loss with respect to each parameter, meaning that the magnitude of the updates is completely independent of the sensitivity of the objective function to those updates. RMSProp applies a similar philosophy to stochastic estimates of the gradient. Letting <span class="math inline">\(\mathbf{g}(t) = \frac{\partial \ell(\theta(t))}{\partial \theta}\)</span></p>
<p><span class="math display">\[\theta_{ij}(t) = \theta_{ij}(t - 1) - \frac{\eta \mathbf{g}_{ij}(t)}{|\mathbf{g}_{ij}(t)|}\]</span></p>
<p>Alternatively, we could write this using matrix notation as</p>
<p><span class="math display">\[\theta(t) = \theta(t - 1) - \frac{\eta \mathbf{g}(t)}{|\mathbf{g}(t)|}\]</span></p>
<p>where the fraction is used to indicate that we are performing element-wise division. The same operation with slightly less notational abuse becomes</p>
<p><span class="math display">\[\theta(t) = \theta(t - 1) - \eta D_{t}\mathbf{g}(t)\]</span></p>
<p>where the matrix <span class="math inline">\(D_{t}\)</span> is diagonal with entries <span class="math inline">\(D_t[i,i]\)</span> equal to <span class="math inline">\(\frac{1}{|\mathbf{g}_{i}(t)|}\)</span>.</p>
<p>This idea of applying some sort of linear transformation to the gradients before computing the update step is common in optimization algorithms. This is because always moving in the direction that locally decreases your loss fastest is not, somewhat counterintuitively, the best way to reduce your loss quickly. On a moment’s reflection, this makes perfect sense. The gradient is telling you to make large updates to your weights in directions where the function is changing rapidly, and small updates in directions where it is changing slowly. If the gradient’s first-order approximation of the function is accurate, then this is a very sensible strategy. If it’s not, then you’re likely to overshoot a minimum in the directions where the gradient is large, and crawl at a snail’s pace in directions where the gradient is small.</p>
<p>To quantify the degree to which a function is changing non-linearly, we use the second derivative, which shows up in the order-2 taylor expansion</p>
<p><span class="math display">\[\ell(x + u) = \ell(x) + \frac{\partial \ell }{\partial x}(x) ^\top u + \frac{1}{2} u^\top \mathbf{H} u\]</span></p>
<p>where <span class="math inline">\(\mathbf{H} = \frac{\partial^2 \ell }{\partial x^2}(x)\)</span>. The eigendecomposition of <span class="math inline">\(\mathbf{H}\)</span> describes directions where gradients are changing quickly (high curvature) and directions where the gradients are changing more slowly (low curvature). It is unfortunately the case that directions where the gradients are changing rapidly (i.e. where the linear approximation given by the gradient is the least accurate) tend also to be directions where the gradients are large. If you recall from my <a href>edge of stability blog post</a>, in order for a gradient descent step to be guaranteed to reduce the loss in this quadratic approximation, we need that the step size <span class="math inline">\(\eta &lt; \frac{2}{\lambda_{\max}(\mathbf{H})}\)</span>. Strictly speaking, between the fact that gradients usually aren’t perfectly aligned with the direction of maximum curvature and other higher-order terms in the loss’s taylor expansion, we usually dont see a neat boundary where step sizes greater than <span class="math inline">\(\frac{2}{\lambda_{\max}(\mathbf{H})}\)</span> strictly diverge, but it’s a useful heuristic.</p>
<p>What this means for optimization, however, is that the step size required to maintain stability in the high curvature directions will potentially be orders of magnitude too small for the optimizer ot make any progress in converging to the optimum along lower-curvature subspaces. It’s easy to construct examples where this is the case – consider a 2-D quadratic function of the form <span class="math inline">\(f(\mathbf{x}) = \mathbf{x}^\top \begin{bmatrix}100 &amp; 0 \\0 &amp; 0.01 \end{bmatrix} \mathbf{x} + \begin{bmatrix}1 \\ 2 \end{bmatrix}^\top \mathbf{x} - 2.\)</span> This function will have gradients <span class="math inline">\(\begin{bmatrix}100 \mathbf{x}_1 + 1 \\ 0.01 \mathbf{x}_2 + 2\end{bmatrix}\)</span>. Its optimum is the vector <span class="math inline">\([0.001, 2000]\)</span>.</p>
<p>Assuming some isotropic gaussian initialization, the component <span class="math inline">\(\mathbf{x}_2\)</span> needs to make its way to a value in the thousands. However, its gradients multiply its value by <span class="math inline">\(0.01\)</span>, meaning that the parameter takes vanishingly small steps relative to <span class="math inline">\(x_1\)</span>, which requires a small learning rate (<span class="math inline">\(\approx 0.02\)</span>) to avoid diverging. The result is that optimization to convergence can take millions of steps to achieve. In convex optimization problems you can derive a precise bound on the number of steps to convergence in terms of the <em>condition number</em> of the Hessian matrix, which is just the ratio <span class="math inline">\(\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}\)</span>, for example yielding the following well-known result from convex optimization (see <a href="https://www.cs.cornell.edu/courses/cs4787/2020sp/lectures/Lecture2.pdf">this</a> helpful set of course notes for more details)</p>
<p><span class="math display">\[ \ell(\theta_T) - \ell^* \leq \exp\bigg( \frac{-T}{\kappa} \bigg) (\ell(\theta_0) - \ell^*).\]</span></p>
<p>The solution to this problem is obvious: don’t take giant steps in directions where you don’t trust your gradient to be an accurate model of the function you’re trying to optimize. The obvious approach to take here is to rescale your gradients proportionally to the model curvature, i.e. replacing the <span class="math inline">\(D\)</span> matrix we saw earlier in RMSProp with <span class="math inline">\(\mathbf{H}^{-1}\)</span>.</p>
<p>This method is known as preconditioned gradient descent. You probably learned about the 1D version of this, <em>Newton’s method</em>, in high school or undergrad. And for the same reasons that Newton’s method results in much faster convergence in root-finding problems, preconditioned gradient descent tends to find local minima much faster in optimization problems.</p>
<p>So that was a long-winded way of saying that pre-conditioners are great, if you can compute them. The challenge in machine learning is that our function inputs don’t have two dimensions, but billions of dimensions, which means any hypothetical Hessian-based pre-conditioner would have billions of billions of entries. Even with Moore’s law going apace, we would have to wait several decades to have computers that can process objects of that size in any reasonable amount of time.</p>
<p>Instead, we use approximations.</p>
<!-- ### Preconditioning in neural networks

**Diagonal pre-conditioners:** the cheapest way of doing something that looks vaguely like pre-conditioning is to re-normalize updates parameter-wise based on the gradient norm, as we saw with RMSProp and SignSGD. There are a variety of ways to go about doing this, for example rescaling gradients on a <a href="">layer-wise basis</a>, or <a href="">parameter-wise</a>. When batches of data are sampled stochastically to give an estimate of the loss function, some sort of exponential-moving-average-based smoothing is typically done, as is the case with Adam. 


**Shampoo:**



**K-FAC:** Kroneker-factored optimization approximates the Hessian as a block-diagonal matrix, taking into account that adjacent weights in the neural network (i.e. weights in adjacent layers)

- Fisher info matrix  -->
<h3 id="the-gauss-newton-approximation">The Gauss-Newton Approximation</h3>
<p>One of the most popular ways of approximating the Hessian is by decomposing it into two matrices, one of which is easy to compute and one of which is hard, then tactfully misplacing the latter. The decomposition can be done in a few lines if you’re willing to play fast and loose with tensor multiplication.</p>
<p>Laying out notation, we assume we have a loss function <span class="math inline">\(\ell: \Theta \rightarrow \mathbb{R}\)</span>, where <span class="math inline">\(\Theta\)</span> denotes the parameter space of a neural network. We further assume that <span class="math inline">\(\ell\)</span> permits the following decomposition: <span class="math inline">\(\ell(\theta) = \ell_f (f(\theta))\)</span>, where <span class="math inline">\(f: \Theta \rightarrow \mathbb{R}^{n \times d}\)</span> denotes the neural network output on some fixed dataset of size <span class="math inline">\(n\)</span> and <span class="math inline">\(\ell_f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}\)</span> is the loss function defined with respect to the neural network outputs, for example <span class="math inline">\(\ell(f(\theta)) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2} (f(\theta) - \mathbf{y}_i)^2\)</span> for regression or <span class="math inline">\(\ell(f(\theta)) = \frac{1}{n}\)</span> <span class="math inline">\(\sum_{i=1}^n \sum_{j=1}^d p_{i, j} \log \frac{\exp{f(\theta)_{i, j}}}{\sum_{j=1}^d \exp{f(\theta)_{i, j}}}\)</span> for cross entropy loss.</p>
<p>With this decomposition, we can rewrite the hessian as follows.
<span class="math display">\[\begin{align}
\mathbf{H} &amp;= \frac{\partial^2}{\partial \theta^2}\ell(\theta) =  \frac{\partial}{\partial \theta} \frac{\partial}{\partial \theta}\ell(\theta) \\
&amp;= \frac{\partial}{\partial \theta}  \frac{\partial}{\partial f} \ell_f(f(\theta)) \frac{\partial}{\partial \theta} f \\
&amp;=  \frac{\partial}{\partial \theta} \delta_\ell \cdot_{n, d}\mathbf{J}_\theta \\
\end{align} \]</span>
<span class="math inline">\(\mathbf{J}_\theta \in \mathbb{R}^{ n \times d\times |\theta|}\)</span> is the network Jacobian, i.e. the matrix of partial derivatives of each output dimension with respect to the parameters, and <span class="math inline">\(\delta_\ell \in \mathbb{R}^{n \times d}\)</span> denotes the derivative of the loss with respect to the network outputs. Finally, we use the magical einstein notation operator <span class="math inline">\(\cdot_{n, d}\)</span> to indicate that we multiplying together and then summing all of the entries in the <span class="math inline">\(n\times d\)</span> output dimensions.</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial}{\partial \theta} (\delta_\ell \cdot_{n,d}\mathbf{J}_\theta) &amp;= (\frac{\partial}{\partial \theta} \delta_\ell) \cdot_{n,d}\mathbf{J}_\theta + \delta_\ell \cdot_{n,d}  \frac{\partial}{\partial \theta} \mathbf{J}_\theta
\end{align}
\]</span>
a non-controversial application of the chain rule gives
<span class="math display">\[
= \frac{\partial }{\partial \theta}\frac{\partial \ell}{\partial f}\delta_\ell \cdot_{n,d} \mathbf{J}_\theta + \delta_\ell \cdot \mathbf{H}_\theta
\]</span>
which we can then decompose by noting that <span class="math inline">\(\frac{\partial}{\partial \theta} = \frac{\partial}{\partial f}\partial{f}{\partial \theta}\)</span></p>
<p><span class="math display">\[
 = \frac{\partial}{\partial f}\frac{\partial f}{\partial \theta}\frac{\partial \ell}{\partial f}\cdot_{n,d} \mathbf{J}_\theta + \delta_\ell \cdot \mathbf{H}_\theta
 \]</span>
since for “nice” functions, you’re allowed to swap the order of the derivative operators, we liberally do so to obtain
<span class="math display">\[
= \frac{\partial f}{\partial \theta}\frac{\partial}{\partial f}\frac{\partial \ell}{\partial f}\cdot_{n,d} \mathbf{J}_\theta + \delta_\ell \cdot \mathbf{H}_\theta
\]</span>
And now we just note that <span class="math inline">\(\frac{\partial {f}}{\partial \theta}\)</span> is exactly the Jacobian <span class="math inline">\(\mathbf{J}_\theta\)</span>, and <span class="math inline">\(\frac{\partial^2}{\partial \theta^2} = \mathbf{H}_{f}\)</span> to get the final decomposition
<span class="math display">\[
=\mathbf{J}_\theta \cdot_{n,d} \mathbf{H}_f \cdot_{n,d} \mathbf{J}_\theta + \delta_\ell \cdot \mathbf{H}_\theta \; .
\]</span></p>
<p>So we have decomposed the Hessian into two terms. The first of these, containing the <span class="math inline">\(\mathbf{H}_f\)</span> term is positive semi-definite and (comparatively) easy to compute as it doesn’t contain any second derivatives with respect to network parameters. The second matrix, by contrast, requires computing the network output Hessian with respect to parameters <span class="math inline">\(\mathbf{H}_\theta\)</span>, and has the impudence to also contain negative eigenvalues.</p>
<center>
<img src="../images/wicked.jpeg" style="margin-left: auto; margin-right: auto;float:clear;" /><br />
<div style="font-size:13px;width=60%;">
The two components of our Hessian decomposition exhibit varying degrees of “fitting convex optimization experts’ ideas of what a loss landscape should look like” and “being possible to physically compute on modern computers”. Much like Elphaba, the unnamed matrix which isn’t the Gauss-Newton matrix is frequently overlooked and misunderstood despite playing an integral role in the training dynamics of… Oz (there are limits to how far one can take a metaphor relating popular culture to second-order optimization methods, unfortunately).
</div>
</center>
<p>The first matrix is also known as the Gauss-Newton matrix, and has a lot of interesting connections to information geometry via the Fisher information matrix, which I might write up as an addendum to this blog post at some point. Much like Elphaba to Galinda, the second matrix is powerful and misunderstood – while it is inconvenient for convex approximations of the local loss landscape, it contains all of the non-convexity of the neural network and is essentially describing the feature-learning component of the dynamics. As the field is beginning to learn, feature learning is actually quite an important component of neural network training.</p>
<p>So that about wraps up this post, which turned into a bit of a scattered collection of observations about first and second derivatives. I hope it was vaguely useful to give some intuition of why a lot of approximate second-order methods end up computing an outer product of gradients instead of the second derivative; perhaps one day in the future I’ll come back to this and write an actually rigorous version of the derivation.</p>
</div></div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="https://scholar.google.co.uk/citations?user=_JMpZh0AAAAJ&hl=en">
                                <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa-brands fa-google-scholar fa-stack-1x fa-inverse"></i>
                            </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://twitter.com/clarelyle">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-brands fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://linkedin.com/in/clarelyle">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-brands fa-linkedin fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://github.com/clareification">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-brands fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>

</body>

</html>
