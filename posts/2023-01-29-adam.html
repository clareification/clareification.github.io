<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content>
    <meta name="author" content>

    <title>Clare Lyle</title>
    <link rel="icon" href="./images/c_icon.png" sizes="32x32" />
    <!-- Bootstrap Core CSS -->
    <link href="../css/bootstrap.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../css/clean-blog.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>
    <link rel="icon" href="./images/c_icon.png" sizes="32x32">
    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="../index.html">Clare Lyle</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="../index.html">Home</a>
                    </li>
                    <li>
                        <a href="../about.html">About</a>
                    </li>
                    <li>
                        <a href="../pubs.html">Publications</a>
                    </li>
                    <li>
                        <a href="../archive.html">Blog</a>
                    </li>
                    <li>
                        <a href="../contact.html">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('/images/robot_scenery_2.png'); box-shadow: inset 0 0 0 1000px rgba(0,0,0,.1); height:70px">
        <div class="container">
            <!-- <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"> -->
<!--                     <div class="site-heading">
                        <h1>Clare Lyle</h1> -->
                        <!--<hr class="small">
                        <span class="subheading">Machine Learning</span>-->
                    <!-- </div> -->
                <!-- </div>
            </div> -->
        </div>
    </header>

    <!-- Main Content -->
    <div class="container"><div class="info" style="font-family:Open Sans; margin-left: 5%">
    Posted on January 29, 2023
</div>
<div class="container">
<h2>When do adaptive optimizers fail to generalize?</h2>
<p><em>A case study</em></p>
<p></p>
<p>There are many reasons to use an adaptive optimizer in machine learning, such as simpler hyperparameter tuning and faster convergence, but improved generalization is usually not one of them. While there have been mixed empirical results concerning whether adam or sgd tends to find solutions that generalize better in deep neural networks, the overall theoretical consensus seems to be that adam is prone to larger generalization gaps than gradient descent. But why should adaptive learning rates lead to solutions that generalize worse? As it turns out, it’s quite easy to construct a simple learning problem where gradient descent finds a solution that generalizes better than adam. This example should provide some intuition about the failure modes of adaptive learning rates and how they might arise in more interesting problems.</p>
<p>The regime of interest is simple: we consider a linear regression problem of <span class="math inline">\(n\)</span> data points <span class="math inline">\(\boldsymbol{x}_i \in \mathbb{R}^{n+1}\)</span>. Letting <span class="math inline">\(e_i\)</span> denote the <span class="math inline">\(i^{th}\)</span> basis vector, we define each data point <span class="math inline">\(\boldsymbol{x}_i, i=1\dots n\)</span> as <span class="math inline">\(e_i + y_i e_{n+1}\)</span>, where <span class="math inline">\(y_i \sim \mathcal{N}(0, 1)\)</span>. We set regression targets equal to <span class="math inline">\(y_i\)</span>, and look for a set of weights <span class="math inline">\(w\)</span> such that <span class="math inline">\(\langle w, \boldsymbol{x}_i \rangle = y_i \; \forall i\)</span>. This is an over-parameterized linear regression problem, and so there are infinitely many solutions. These solutions have the interpretation of being some combination of two archetype solutions: the first is what I’ll call the ‘generalizing’ solution, and is of the form <span class="math inline">\(w_G = (0, \dots, 0, 1)\)</span>. The second is the ‘memorizing’ solution <span class="math inline">\(w_M\)</span> and has the form <span class="math inline">\(\frac{1}{y_1}, \dots, \frac{1}{y_n}, 0\)</span>.</p>
<p>This problem is constructed to clearly distinguish between features that generalize between data points and features which `memorize’, and only contain data relevant to the data point where they are active. Here the feature <span class="math inline">\(e_{n+1}\)</span> is constructed to contain all of the generalizable information between datapoints. While it doesn’t necessarily make sense to imagine how <span class="math inline">\(w_G\)</span> will generalize to new data, we note that for a subset <span class="math inline">\(S = \{x_{i_1}, \dots, x_{i_k}\} \subset \boldsymbol{X}\)</span>, the induced memorizing solution <span class="math inline">\(w_M^S = \sum_{i_k} \frac{1}{y_{i_k}} e_{i_k}\)</span> would obtain generalization error equal to <span class="math display">\[\sum_{y_i \in S^C} y_i^2 \]</span> on the remaining data points.</p>
<p>So what happens when we run gradient descent on this problem? For simplicity, we consider full-batch gradient descent with fixed step-size <span class="math inline">\(\alpha\)</span>. In this case, ignoring the negligible feature noise, we get
<span class="math display">\[\nabla_w \frac{1}{2} \|\boldsymbol{X}w - y\|^2 = X^\top(\boldsymbol{X}w - y)\]</span>
when <span class="math inline">\(w=0\)</span>, this becomes
<span class="math display">\[\nabla_w \frac{1}{2} \|\boldsymbol{X}w - y\|^2 = (y_1, \dots, y_n, \sum_{i=1}^n y_i^2)\]</span></p>
<p>If <span class="math inline">\(y_i\)</span> is very small, then it is possible that <span class="math inline">\(\sum_{i=1}^n y_i^2\)</span> can in principle be smaller than <span class="math inline">\(y_i\)</span>, however if we assume that the features are reasonably balanced, then in most cases the update to coefficient <span class="math inline">\(n+1\)</span> will be much larger than the updates to the other coefficients. Intuitively, this is because there are <span class="math inline">\(n\)</span> data points which are all contributing mass to this dimension, whereas each other dimension is only influenced by a single data point. A similar line of argument applies to future gradient steps, and the end result is that gradient descent will tend to bias towards solutions which are more similar to <span class="math inline">\(w_G\)</span> than <span class="math inline">\(w_M\)</span>. Here’s the result I got by running gradient descent on the problem described above.</p>
<div data-align="center">
<img width="40%" src="../images/sgd_solution.png" /> <br />.
</div>
<p>Now consider adam.</p>
<p>To simplify our job, we’ll actually consider RMSProp, as adam has a number of bells and whistles that would just distract us from our main object of interest: the adaptive step size. RMSProp is a batched version of a sign gradient method, where the idea is to perform an equal-sized update on all parameters, independent of the magnitude of the gradient associated with each one. For example, if <span class="math inline">\(w_1\)</span> has a gradient of <span class="math inline">\(5\)</span> and <span class="math inline">\(w_2\)</span> has a gradient of <span class="math inline">\(0.001\)</span>, they both get the same update of <span class="math inline">\(\alpha\)</span>. This has some intuitive justification: if the gradient is small then you need to take a larger step to have a similar effect on the loss as if the gradient were large. Because we usually work with estimates of the gradient taken on minibatches, methods like RMSProp keep track of a running estimate of the gradient magnitude, and scale updates by this estimate to get a batched version of the uniform-update rule. For more details, see e.g. Geoff Hinton’s notes. If you do read through these notes, you might notice that while there does now exist some theoretical analysis showing that this type of method can converge, this analysis was decidedly not the motivating factor in the design of the algorithm.</p>
<p>Adam and RMSProp keep track of two quantities: a running gradient average <span class="math inline">\(\widehat{g}_t\)</span>, and a secon dmoment estimator <span class="math inline">\(\widehat{v}_t\)</span>. Given a sample gradient <span class="math inline">\(g_t\)</span>, we get updates <span class="math inline">\(\widehat{g}_t = (1-\beta_1)g_t + \beta_1 \widehat{g}_{t-1}\)</span>, and <span class="math inline">\(\widehat{v}_t = (1-\beta_1)g_t^2 + \beta_2 \widehat{v}_{t-1}\)</span>. We end up with the following update rule, where <span class="math inline">\(\epsilon\)</span> is some error tolerance term to avoid dividing by zero:</p>
<p><span class="math display">\[ w_{t+1} = w_t - \alpha \frac{\widehat{g}_t}{\sqrt{\widehat{v}_t} + \epsilon} \]</span></p>
<p>The important thing to note here is that if the magnitude of the gradient of some parameter is zero for the vast majority of inputs, then when we finally do see a nonzero gradient, the effective step size will be enormous relative to the parameters that are updated more frequently.</p>
<p>What implications does this have for the generalization of the parameters that an adaptive optimizer will find in our toy problem? Well, under the non-batched RMSProp update rule, the gradient magnitudes for generalizing and non-generalizing indices will be exactly equal. As a result, we get updates of the form <span class="math inline">\((\pm 1 )_{i=1}^{n+1}\)</span>. In this sense, we can say that <em>by design</em> the optimizer doesn’t have a preference for features that appear in many data points compared to features which only arise in a small subset of samples.</p>
<p>As a result, when we run an adam optimizer on the same problem as we used to generate the previous figure, we get the following:</p>
<div data-align="center">
<img width="40%" src="../images/adam_solution.png" /> <br />.
</div>
<p>To test how these solutions generalize, we can imagine partitioning the dataset into a train and test split. Because of how we’ve constructed the data, naively taking the features from the original problem will be a less informative experiment than one might initially expect. Several indices of the features will be zero for <em>all</em> training inputs, meaning that there is no gradient for the optimizer to reduce whatever initial weight was assigned to these indices, and so both GD and adam should generalize similarly based on their initialization. If we add a small amount of noise to the features, so that we define each data point <span class="math inline">\(\boldsymbol{x}_i, i=1\dots n\)</span> as <span class="math inline">\(e_i + y_i e_{n+1} + \epsilon_{i}\)</span>, where <span class="math inline">\(y_i \sim \mathcal{N}(0, 1)\)</span>, with <span class="math inline">\(\epsilon_{i} \in \mathbb{R}^{n+1} \sim \epsilon \mathcal{N}(0, Id)\)</span> for some small but nonzero <span class="math inline">\(\epsilon\)</span>, then the zero-gradients issue goes away while preserving the intuition of the original problem setting that one feature is significantly more predictive of the outcome than any other.</p>
<p>After training with gradient descent and adam, we essentially replicate the result above: gradient descent assigns a high magnitude to the generalizing feature and low magnitudes to the rest. Adam assigns large magnitudes all over the place. However, adam converges faster and gets lower training error after a truncated training period of 1000 steps. We might be a bit conflicted then on which solution we should pick – in previous papers I did suggest that training speed should be indicative of generalization in at least some cases. To give some hint as to which way the answer to this question will go, I present without further comment the weights learned by adam and sgd on this problem.</p>
<div data-align="center">
<img width="30%" src="../images/noisy_adamsgd_weights.png" /> <br />
</div>
<p>To evaluate how well the two solutions generalize, I looked at what happened when training on 100 points of a 200-datapoint dataset and then evaluating both SGD and Adam on the entire set of points. The result is as would be expected: while adam is perfectly able to fit its training set, the high weight magnitude to somewhat random indices elsewhere contribute to gigantic errors on the test points.</p>
<div data-align="center">
<img width="30%" src="../images/noisy_adamsgd_train.png" /> <img width="30%" src="../images/noisy_adamsgd_test.png" />
</div>
<p>Obviously, this setting is a bit contrived. For starters, it will be extremely rare in naturally occuring data for the principal component to be precisely axis-aligned with a single parameter. Adam’s second-moment correction is axis-aligned, and so the situation constructed in this blog post is probably the absolute worst-case setting you could put adam into. I tried replicating the analysis described in this blog post with non-axis-aligned features, and the result was much closer to what is reported in practice: gradient descent exhibited slightly better generalization, but nothing crazy like in the example here. Generally speaking, while it seems like adaptive learning rates have the potential to catastrophically affect generalization, in practice they don’t because they’re not very good at approximating curvature when features aren’t axis-aligned.</p>
</div></div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="https://scholar.google.co.uk/citations?user=_JMpZh0AAAAJ&hl=en">
                                <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa-brands fa-google-scholar fa-stack-1x fa-inverse"></i>
                            </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://twitter.com/clarelyle">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-brands fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://linkedin.com/in/clarelyle">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-brands fa-linkedin fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://github.com/clareification">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-brands fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>

</body>

</html>
