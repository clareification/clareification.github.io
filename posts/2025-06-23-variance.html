<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content>
    <meta name="author" content>

    <title>Clare Lyle</title>
    <link rel="icon" href="./images/c_icon.png" sizes="32x32" />
    <!-- Bootstrap Core CSS -->
    <link href="../css/bootstrap.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../css/clean-blog.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>
    <link rel="icon" href="./images/c_icon.png" sizes="32x32">
    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="../index.html">Clare Lyle</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="../index.html">Home</a>
                    </li>
                    <li>
                        <a href="../about.html">About</a>
                    </li>
                    <li>
                        <a href="../pubs.html">Publications</a>
                    </li>
                    <li>
                        <a href="../archive.html">Blog</a>
                    </li>
                    <li>
                        <a href="../contact.html">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('/images/robot_scenery_2.png'); box-shadow: inset 0 0 0 1000px rgba(0,0,0,.1); height:60px">
        <div class="container">
            <!-- <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"> -->
<!--                     <div class="site-heading">
                        <h1>Clare Lyle</h1> -->
                        <!--<hr class="small">
                        <span class="subheading">Machine Learning</span>-->
                    <!-- </div> -->
                <!-- </div>
            </div> -->
        </div>
    </header>

    <!-- Main Content -->
    <div class="container"><div class="info" style="font-family:Open Sans; margin-left: 5%">
    Posted on June 23, 2025
</div>
<div class="container">
<h2>Why do we divide by n-1 when estimating the variance?</h2>
<p><em>Two answers to the question every first-year stats student asks</em></p>
<p></p>
<p>In every introductory statistics course, we learn the following estimators of the mean and variance of some distribution <span class="math inline">\(P\)</span> from which we have <span class="math inline">\(n\)</span> samples <span class="math inline">\(x_1, \dots, x_n\)</span>.</p>
<p>The first moment <span class="math inline">\(\mu_1\)</span>, i.e. the mean <span class="math inline">\(\mathbb{E}[X]\)</span> is straightforward to estimate from samples as</p>
<p><span class="math display">\[\widehat{\mu_1} = \frac{1}{n} \sum_{i=1}^n x_i\]</span></p>
<p>For the second moment, i.e. <span class="math inline">\(\mathbb{E}_{X \sim P}[X^2]\)</span> (this doesn’t have a standardized notation like the mean, so I’ll call it <span class="math inline">\(\mu_2\)</span> for ‘moment 2’) we also have the straightforward estimator</p>
<p><span class="math display">\[\widehat{\mu_2} = \frac{1}{n} \sum_{i=1}^n x_i\]</span></p>
<p>But the variance $[(X - [X])^2] sometimes catches people by surprise. Naive pattern-matching would suggest its estimator would be</p>
<p><span class="math display">\[\widehat{\nu_2} = \frac{1}{n} \sum_{i=1}^n (x_i - \frac{1}{n}\sum_{i=1}^n x_i)^2\]</span></p>
<p>which is in fact wrong. Well, technically I don’t know if an estimator can be <em>wrong</em> as that’s a bit of a subjective judgement, but it is definitely biased and will result in you under-estimating the actual variance. There are a number of ways to show that this is wrong, but the simplest is to take <span class="math inline">\(n=1\)</span>, i.e. the case where we only have a single sample from the distribution. Then the quantity <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n (x_i - \frac{1}{n}\sum_{i=1}^n x_i)^2 = 0\)</span> no matter what the distribution we are sampling from. This is not a desirable property in an estimator, so we can safely discard the naive <span class="math inline">\(\frac{1}{n}\)</span> factor.</p>
<p>We’ve shown that <span class="math inline">\(\frac{1}{n}\)</span> is wrong, but we haven’t shown why it fundamentally should be wrong, nor have we shown that the <span class="math inline">\(\frac{1}{n-1}\)</span> scaling factor is <em>correct</em>. I’ll get to the final point in a moment, but it’s worth meditating for a moment on why dividing by <span class="math inline">\(n\)</span> isn’t the right thing to do. We take an average over measurements when we think that those measurements are, on average (pardon the redundancy), capturing the underlying quantity they are measuring. The mean estimator, for example, averages out a collection of unbiased estimates of the expected value by averaging over samples. From another perspective, the sample mean is the point that minimizes the average distance to sampled points. But this average distance from the distribution’s expected value <span class="math inline">\(\mu\)</span> to sampled points is the variance that we want to estimate. Consequently, if we use the sample mean to estimate the average distance between sampled points and the <em>actual</em> expected value of the distribution, we are going to get an under-estimate.</p>
<p>I’ll now show two ways of proving that the factor by which the sample variance gives an under-estimate is precisely the <span class="math inline">\(\frac{n-1}{n}\)</span> factor that results in us needing to divide by <span class="math inline">\(n-1\)</span> to get an unbiased estimator.</p>
<h3 id="proof-1-decompose-the-sum">Proof 1: decompose the sum</h3>
<p>A simple way to show that you need to divide by <span class="math inline">\(\frac{1}{n-1}\)</span> to get an unbiased estimate of a distribution’s variance is to compute the expected value of the sample variance explicitly and see if it’s off by a factor of <span class="math inline">\(\frac{n-1}{n}\)</span>. Recall, we want to compute</p>
<p><span class="math display">\[\sigma^2 = \mathbb{E}[(x_i - \mu_1)^2]\]</span></p>
<p>If we know <span class="math inline">\(\mu_1\)</span>, then</p>
<p><span class="math display">\[\widehat{\nu_2} = \frac{1}{n} \sum_{i=1}^n (x_i - \mu_1)^2 \]</span></p>
<p>is an unbiased estimator, which is easy to show as</p>
<p><span class="math display">\[\mathbb{E}[\widehat{\nu_2}] = \mathbb{E}[\frac{1}{n} \sum_{i=1}^n (x_i - \mu_1)^2] =\frac{1}{n} \sum_{i=1}^n \mathbb{E}[(x_i - \mu_1)^2] = \mathbb{E}[(x_i - \mu_1)^2] \]</span></p>
<p>If we don’t know the mean, we have to use our best guess.</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}[\widehat{\nu_2}] &amp;= \mathbb{E}[\frac{1}{n} \sum_{i=1}^n (x_i - \widehat{\mu}_1)^2] \\
&amp; =\frac{1}{n}  \sum_{i=1}^n \mathbb{E}[x_i^2 - 2x_i \widehat{\mu_1} + \widehat{\mu_1}^2 ] \\
&amp;= \mathbb{E}[x_i^2 -2x_i \widehat{\mu_1} + \widehat{\mu_1}^2 ] \\
&amp;= \mathbb{E}[x_i^2] -2\mathbb{E}[x_i \widehat{\mu_1}] + \mathbb{E}[\widehat{\mu_1}^2 ] \\
\end{align}
\]</span>
Now, unfortunately for us we can’t compute <span class="math inline">\(\mathbb{E}[x_i \widehat{\mu_1}]\)</span> as <span class="math inline">\(\mathbb{E}[x_i ]\mathbb{E}[\widehat{\mu_1}]\)</span> because the estimator <span class="math inline">\(\mu_1\)</span> depends on the value of <span class="math inline">\(x_i\)</span>. So instead we have to divvy up the sum into a set of independent variables where we do get <span class="math inline">\(\mathbb{E}[x_ix_j] = \mathbb{E}[x_i] \mathbb{E}[x_j] = \mathbb{E}[X]^2\)</span>, and self-product terms of the form <span class="math inline">\(\mathbb{E}[x_i^2]\)</span>.
<span class="math display">\[
\begin{align}
\mathbb{E}[\widehat{\nu_2}] &amp;= \mathbb{E}[x_i^2] -2\mathbb{E}[\frac{1}{n}\sum_{j=1}^n x_ix_j] + \mathbb{E}[(\frac{1}{n}\sum_{i=1}^n x_i)^2] \\
&amp;= \mathbb{E}[x_i^2] -2(\mathbb{E}[\frac{1}{n}\sum_{j \neq i}x_ix_j] + \frac{1}{n}\mathbb{E}[x_i^2]) + \frac{1}{n^2}(n\mathbb{E}[\sum_{i=1}^{n-1}x_nx_i] + n\mathbb{E}[x_n^2]) \\
&amp;= \mathbb{E}[x_i^2] -2(\frac{n-1}{n}\mathbb{E}[x_i]^2 + \frac{1}{n}\mathbb{E}[x_i^2]) + \frac{n-1}{n}\mathbb{E}[x_i]^2 + \frac{1}{n}\mathbb{E}[x_i^2] \\
&amp;= \frac{n-1}{n} [\mathbb{E}[X_i^2] - \mathbb{E}[X_i]^2]
\end{align}
\]</span></p>
<h3 id="proof-2-use-the-error-of-the-sample-mean-estimator">Proof 2: use the error of the sample mean estimator</h3>
<p>Another intuition we can use instead is that the estimated sample mean will not be exactly equal to the true mean, and will by construction be closer to the sampled points than the population mean. We can compute the expected error of the sample mean estimator as</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}[(\mu - \widehat{\mu}_1)^2] &amp;=  \mathbb{E}[(\mu - \frac{1}{n} \sum_{i=1}^n x_i)^2] \\
&amp;= \mathbb{E}[\mu^2 - 2\mu\sum_{i=1}^n \frac{x_i}{n} + (\frac{1}{n} \sum_{i=1}^n x_i)^2]\\
&amp;= \mu^2 - 2\mathbb{E}[x_i]\mu + \mathbb{E}[\frac{1}{n^2}(\sum_{i=1}^n x_i)^2] \\
&amp;= -\mu^2 + \mathbb{E}[\sum_{i=1}^n \sum_{j \neq i} x_ix_j + n\sum_{i=1}^n x_i^2] \\
&amp;=  -\mu^2 + \frac{1}{n^2}(\sum_{i=1}^n \sum_{j \neq i} \mathbb{E}[x_i]^2 + \sum_{i=1}^n \mathbb{E}[x_i^2] )\\
&amp;= -\mu^2 \frac{n-1}{n} + \mu^2 + \frac{1}{n}[\mu_2] \\
&amp;= \frac{ \mu_2 - \mu^2}{n} \\
&amp;= \frac{\sigma^2}{n}
\end{align}
\]</span></p>
<p>So we see that the estimated mean will be off by an average of <span class="math inline">\(\frac{\sigma^2}{n}\)</span>, a factor we need to account for if we want to estimate the average distance between sample points and the true mean. In particular, this leaves us with
<span class="math display">\[
\begin{align}
\sigma^2 = \mathbb{E}[\frac{1}{n}\sum_{i=1}^n(\mu -x_i)^2 ] &amp;= \mathbb{E}[\frac{1}{n}\sum_{i=1}^n({\color{blue}{\mu - \widehat{\mu}}} + {\color{red}{\widehat{\mu} - x_i}})^2] \\
&amp;= \mathbb{E}[\frac{1}{n}\sum_{i=1}^n(({\color{blue}{\mu - \widehat{\mu}}})^2 + 2({\color{blue}{\mu - \widehat{\mu}}})(\color{red}{\widehat{\mu} - x_i}) + (\color{red}{\widehat{\mu} - x_i})^2)] \\
&amp;= \frac{\sigma^2}{n}+ \frac{2}{n}\mathbb{E}[\sum_{i=1}^n({\color{blue}{\mu - \widehat{\mu}}})({\color{red}{\widehat{\mu} - x_i}})] + \mathbb{E}[(\color{red}{\widehat{\mu} - x_i})^2] \\
\sigma^2 &amp;=  \frac{\sigma^2}{n} + 0 + \mathbb{E}[(\color{red}{\widehat{\mu} - x_i})^2] \\
\implies \mathbb{E}[\frac{1}{n}\sum_{1=1}^n(\color{red}{\widehat{\mu} - x_i})^2] &amp;= \sigma^2 - \frac{\sigma^2}{n} = \frac{n-1}{n}\sigma^2
\end{align}
\]</span></p>
<p>where we have that the term</p>
<p><span class="math inline">\(\sum_{i=1}^n((\mu - \widehat{\mu}))(\widehat{\mu} - x_i) = ((\mu - \widehat{\mu}))\sum_{i=1}^n(\color{red}{\widehat{\mu} - x_i}) =((\mu - \widehat{\mu}))(n \widehat{\mu} - \sum x_i) = 0\)</span></p>
<h3 id="concluding-remarks">Concluding remarks</h3>
<p>So there you have it: two different perspectives on why we divide the sample variability by <span class="math inline">\(n-1\)</span> instead of <span class="math inline">\(n\)</span> to estimate the variance. From the random variable perspective, the missing <span class="math inline">\(\frac{1}{n}\)</span> term comes from the fact that the non-zero component of the correlation between the random sample <span class="math inline">\(x_i\)</span> and the estimated mean <span class="math inline">\(\frac{1}{n}\sum_{j=1}^n{x_j}\)</span> comes from the <span class="math inline">\(\frac{1}{n}\mathbb{E}[x_i^2]\)</span> term, and this correlation will reduce the observed variance. From a predictive accuracy perspective, we know that the expected error of the mean estimator is <span class="math inline">\(\frac{\sigma^2}{n}\)</span> and the direction of this error will, by construction, always be closer to your data than the actual mean and so lead to an underestimation of the variance.</p>
<p>I’m assuming universities, slow-moving as they are, haven’t updated their curricula to reflect that AI is now distributed systems engineering (as opposed to statistics, or before that search algorithms) and are still teaching statistics, so I hope this diversion was helpful to the odd student who stumbles on my website. Otherwise, it was at least a fun excuse for me to write multi-line equations for the first time in a very long while, and I thank whatever readers this blog has for putting up with my indulgences.</p>
</div></div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="https://scholar.google.co.uk/citations?user=_JMpZh0AAAAJ&hl=en">
                                <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa-brands fa-google-scholar fa-stack-1x fa-inverse"></i>
                            </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://twitter.com/clarelyle">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-brands fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://linkedin.com/in/clarelyle">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-brands fa-linkedin fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://github.com/clareification">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-brands fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>

</body>

</html>
