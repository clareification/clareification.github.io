<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content>
    <meta name="author" content>

    <title>Clare Lyle</title>
    <link rel="icon" href="./images/c_icon.png" sizes="32x32" />
    <!-- Bootstrap Core CSS -->
    <link href="../css/bootstrap.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../css/clean-blog.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>
    <link rel="icon" href="./images/ox.png" sizes="32x32">
    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="../index.html">Clare Lyle</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="../index.html">Home</a>
                    </li>
                    <li>
                        <a href="../about.html">About</a>
                    </li>
                    <li>
                        <a href="../pubs.html">Publications</a>
                    </li>
                    <li>
                        <a href="../archive.html">Blog</a>
                    </li>
                    <li>
                        <a href="../contact.html">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('/images/seattle.jpg'); box-shadow: inset 0 0 0 1000px rgba(0,0,0,.1); height:100px">
        <div class="container">
            <!-- <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"> -->
<!--                     <div class="site-heading">
                        <h1>Clare Lyle</h1> -->
                        <!--<hr class="small">
                        <span class="subheading">Machine Learning</span>-->
                    <!-- </div> -->
                <!-- </div>
            </div> -->
        </div>
    </header>

    <!-- Main Content -->
    <div class="container"><div class="info" style="font-family:Open Sans; margin-left: 10%">
    Posted on October 30, 2020
</div>
<div class="container">
<h1>A Bayesian Perspective on Training Speed and Model Selection</h1>
<style>
button {
	background-color:white;
	border:3px solid black;
	
}
.figure {
	font-size:small;
	margin: 2px;
}
</style>
<script>function hideDiv(divid) {
  var x = document.getElementById(divid);
  console.log(divid);
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}</script>
<p><strong>TL;DR</strong> <strong>Models that train faster</strong> (with respect to the number of data points they need to fit the dataset) have a <strong>higher marginal likelihood</strong>. We leverage this to get estimators of the (log) marginal likelihood in linear models that depend on the <strong>sum of training losses</strong> obtained in an iterative updating procedure. We show an intriguing connection between these estimators and the weight assigned to features in linear regression, and find that <strong>the intuition driving these estimators also seems to hold in deep neural networks</strong>.</p>
<p>#Introduction</p>
<p>Suppose that you’re trying to build a <a href="https://www.youtube.com/watch?v=pqTntG1RXSY">hotdog classifier</a>, and you have a choice between two neural networks: network A has a cross-entropy loss of 0.01 on the training set, and it reached that loss extremely quickly. Network B obtains a cross-entropy loss of 0.0001 on the training set, but it took a lot longer to get below 0.01 than Network A. You want to use the model that will be the most accurate on new, unseen possible-hotdogs: which do you pick?</p>
<div class="figure" style="width:40%; margin:2px; float:right;">
<img width="90%" src="../images/hotdog_classifier.png" /> <br /> <br />
A difficult model selection problem.
</div>
<p>If you held out some training data for a validation set, then you can pick the one with better performance on the unseen data and be done. But suppose you don’t have access to a validation set. Is there still a way of justifying the intuition that the model that got a pretty low loss quickly is less likely to have overfit than the model that got an extremely low loss slowly? It turns out the answer is yes – if you’re willing to be Bayesian.</p>
<h2 id="background-on-model-selection">Background on Model Selection</h2>
<p><strong>TL;DR</strong></p>
<ul>
<li>Model selection and generalization are deeply connected</li>
<li>Generalization in neural networks is not well-understood</li>
<li>There are some generalization bounds that are related to training speed</li>
<li>A Bayesian approach to model selection is to pick the model with the highest marginal likelihood</li>
</ul>
<button onclick="hideDiv('background-div')">
Toggle details
</button>
<div id="background-div" style="display:none;">
<p>###Model selection in neural networks is hard.</p>
<p>Before digging into the Bayesian perspective, we’re going to give some context for the model selection problem in deep learning. In most machine learning applications, our goal is to produce some function approximator that fits a target function on some unknown data-generating distribution, from which we have samples in the form of a training set. Typically, some points from this training set is hidden from the model during training, and an estimate of the model’s expected risk (its average error on the data generating distribution) is obtained by computing the validation loss on this held-out data. The model with the lowest validation loss is assumed to also have the lowest expected risk. While the validation loss is great as an estimator of empirical risk, it’s not ideal for hyperparameter tuning because a) it’s expensive to optimize and b) it doesn’t explain <a href="https://arxiv.org/abs/2010.11924"><em>why</em> a model will generalize well</a>.</p>
<p>Generalization bounds can be used to address both of these issues. A generalization bound gives a high-probability guarantee that the test set error will not exceed a certain value, and can typically be decomposed into an empirical risk estimate (i.e. the training loss) and a model complexity penalty. The model complexity penalty can be interpreted as giving a causal hypothesis for what makes a model generalize well, addressing point a) above. When both of these terms are differentiable, the bound can be <a href="https://arxiv.org/abs/1703.11008">optimized directly</a>, addressing point b) above. While generalization bounds can get <a href="https://arxiv.org/abs/1804.05862">non-vacuous</a> values on neural networks, they’re generally <a href="https://arxiv.org/abs/1912.02178">not tight enough</a> to be useful for model selection. We conjecture that part of the reason these bounds might struggle so much in the deep learning regime is that the model complexity term used typically only depends on the final value of the model’s parameters after training, rather than taking into account properties of the training scheme. Some <a href="https://papers.nips.cc/paper/2019/file/05ae14d7ae387b93370d142d82220f1b-Paper.pdfs">recent work</a> has developed PAC-Bayesian generalization bounds based on a similar idea, but there’s likely much more that could be explored.</p>
<p>The idea that models which train faster should generalize better is not a new one: generalization bounds based on the <em>stability</em> of gradient descent have existed <a href="https://arxiv.org/abs/1509.01240">for</a> <a href="https://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf">years</a>. These bounds explicitly depend on the number of training steps taken to reach a minimum. However, they also require some assumptions that aren’t obvious to show in the generic deep learning setting. There have also been some more oblique connections in the literature. For example, <a href="https://arxiv.org/abs/1901.08584">Arora et al.</a> propose a generalization bound with a data complexity term that can be related to an upper bound on the convergence rate of gradient descent on a convex function.</p>
<p>Instead of trying to directly solve the model selection problem for neural networks, we’re going to take a digression into an easier setting, Bayesian models, and develop some theoretically grounded intuition on the model selection there. Don’t forget about this section though – we’ll come back to it in a bit.</p>
<p>###Bayesian Inference in 2 Minutes</p>
<p>Being a Bayesian is all about using Bayes’ rule to update prior beliefs based on evidence. For example, given a model <span class="math inline">\(M\)</span> and some data <span class="math inline">\(D\)</span>, our belief that the model generated the data can be expressed in terms of the likelihood the model assigns the data, and our prior probability of the data and the model.</p>
<p><span class="math display">\[P(M|D) = \frac{P(D|M)P(D)}{P(M)}\]</span></p>
<p>To quantify terms like ‘evidence’ and ‘prior belief’, we translate some standard quantities from the risk minimization framework into probabilistic analogues. Instead of optimizing the parameters of a function using its loss on the data, we have a model <span class="math inline">\(\mathcal{M}\)</span> which defines a probability distribution over the data <span class="math inline">\(P(\mathcal{D}|\theta)\)</span> given some parameters <span class="math inline">\(\theta \in \Theta\)</span>, along with a probability distribution over the parameters <span class="math inline">\(P(\theta)\)</span>.</p>
<p>One quantity of interest is to Bayesians is the posterior distribution over parameters <span class="math inline">\(P(\theta|\mathcal{D})\)</span>: finding this distribution is known as parameter inference. While this computation is straightforward in some models like Gaussian Processes, it’s often not tractable to compute exactly for complex models like Bayesian neural networks. However, it’s still possible to get samples from this distribution via techniques such as ensemble methods, where we randomly draw a number of parameter samples from the prior and then optimize each parameter sample independently to get a sample from the posterior.</p>
<p>The quantity that we will be particularly interested in is the <em>marginal likelihood</em> <span class="math inline">\(P(\mathcal{D})\)</span> (we’ll abbreviate this as ML from here on out). This tells us how likely the data is under the model, and can be written as follows.</p>
<p><span class="math display">\[P(\mathcal{D}) = \int_{\theta} P(\mathcal{D}|\theta) P(\theta) d\theta \]</span></p>
<p>If we have a collection of models, picking the one with the highest marginal likelihood is what MacKay calls Type II maximum likelihood estimation and is a popular approach to Bayesian model selection. Maximizing the marginal likelihood is less prone to overfitting than maximum likelihood estimation at the parameter level, as the marginal likelihood can be viewed as having a built-in model complexity term that helps to prevent overfitting.</p>
</div>
<p>#Estimating the Marginal Likelihood</p>
<h3 id="decomposing-the-log-ml">Decomposing the log ML</h3>
<p>OK, so if we want to perform model selection by maximizing the marginal likelihood, we’ll need to compute the marginal likelihood first. Observe that we can write the (log) ML as follows.</p>
<p><span class="math display">\[\log P(\mathcal{D}) = \sum \log P(\mathcal{D}_i|\mathcal{D}_{&lt;i}) \]</span></p>
<p>Making the dependence of the conditional <span class="math inline">\(P(D_i|D_{&lt;i})\)</span> on the parameters <span class="math inline">\(\theta\)</span> more explicit, we get</p>
<p><span class="math display">\[\log P(\mathcal{D}) = \sum \log \mathbb{E}_{P(\theta | \mathcal{D}_{&lt;i})}[P(\mathcal{D}_i | \theta)] \]</span></p>
<p>In other words: the marginal likelihood measures how well a posterior update based on one subset of the data <span class="math inline">\(\mathcal{D}_{&lt;i}\)</span> is able to predict the next data point <span class="math inline">\(\mathcal{D}_i\)</span>. We can visualize the log ML as computing the ‘area under the curve’ of posterior predictive probabilities.</p>
<div class="figure">
<p><img src="../images/ml-figure.png" width="80%" /></p>
</div>
<h3 id="estimator-zoo">Estimator Zoo</h3>
<p>The value <span class="math inline">\(\log \mathbb{E}[P(\mathcal{D}_i|\theta)]\)</span> isn’t always possible to compute exactly. However, given parameter samples from the posterior, it’s straightforward to estimate a lower bound.</p>
<p><span class="math display">\[\log P(\mathcal{D}) = \sum \log \mathbb{E}_{P(\theta | \mathcal{D}_{&lt;i})}[P(\mathcal{D}_i | \theta)]  \ge \sum \mathbb{E}_{P(\theta | \mathcal{D}_{&lt;i})}[  \log P(\mathcal{D}_i | \theta)]  = L(D)\]</span></p>
<p>Where the inequality follows from Jensen’s inequality. We can make this inequality tighter by averaging over multiple parameter samples before applying the logarithm to get another estimator that we’ll call <span class="math inline">\(L_k\)</span>.</p>
<p><span class="math display">\[ \log P(\mathcal{D}) \geq  \sum \mathbb{E}_{P(\theta | \mathcal{D}_{&lt;i})} \; [  \log \frac{1}{k} \sum_{j=1}^k P(\mathcal{D}_i | \theta_j)]   = L_k(D)\]</span></p>
<p>Finally, if we have samples from the posterior <em>predictive</em> distribution <span class="math inline">\(P(\hat{D_i} | D_{&lt;i})\)</span> and this distribution is a Gaussian, then we can use our posterior samples to estimate the parameters <span class="math inline">\(\mu, \sigma^2\)</span> of <span class="math inline">\(P(\cdot | D_{&lt;i})\)</span> and then use those estimated parameters in the log likelihood term to obtain yet another estimator <span class="math inline">\(L_S\)</span>.</p>
<p><span class="math display">\[ \log P(\mathcal{D}) = \sum_{i=1}^n \mathbb{E}[\log P(\mathcal{D}_i|\widehat{\mu}_i, \widehat{\sigma}^2_i)]  = L_S(D)\]</span></p>
<p>Each of these estimators has its own pros and cons: <span class="math inline">\(L\)</span> has an intriguing interpretation from the minimum description length framework, but can have a large bias term when used to estimate the ML. <span class="math inline">\(L_k\)</span> reduces this bias term, but both <span class="math inline">\(L_k\)</span> and <span class="math inline">\(L\)</span> can’t be applied to models whose likelihood is a dirac delta distribution (i.e. models with zero observation noise). <span class="math inline">\(L_S\)</span> works even for models with zero observation noise, but only yields an unbiased estimate of a lower bound when the posterior predictive is Gaussian (otherwise this may no longer be a lower bound).</p>
<h3 id="estimating-the-log-ml-via-gradient-descent">Estimating the log ML via Gradient Descent</h3>
<div class="figure" style="width:50%; float:right;">
<img width="90%" style="float:right;" src="../images/linear-ml-alg.png" />
</div>
<strong>TL;DR:</strong> In linear models, we can sample parameters from a prior and then run gradient descent to get a posterior sample to feed into the estimators from the last section.
<button onclick="hideDiv('graddescent')">Toggle details.</button>
<div id="graddescent" style="display:none;">
<p>In linear models, we can get the posterior samples that these estimators need by the ensemble methods described previously. Specifically: in Bayesian linear regression with noiseless Gaussian likelihoods, we can sample parameters from the prior and then optimize the <span class="math inline">\(\ell_2\)</span> regression loss. The minimum of this optimization problem will correspond to a sample from the posterior of the model conditioned on the data used for optimization. This gives us a way of translating Bayesian updating to gradient optimization with ensembles, which we can adapt to models with noisy likelihoods (assuming we know the ground-truth targets) by adding noise to the targets and optimizing a penalized regression loss which encourages parameters to stay close to their initialized values.</p>
<div class="figure" style="width:30%; float:right;">
<img width="90%" style="float:right;" src="../images/gaussian.png" /> <br /> Optimizing a linear regression objective lets us sample from the posterior.
</div>
<ul>
<li>
randomly initialize some collection of parameters <span class="math inline">\((\theta_0^k)\)</span>
</li>
<li>
for each <span class="math inline">\(D_i \in D\)</span>, run GD on <span class="math inline">\(D_{&lt;i}\)</span> to convergence for each parameter <span class="math inline">\(\theta^k_{i-1}\)</span>
</li>
<li>
take this parameter <span class="math inline">\(\theta^k_i \sim p(\theta|D_{&lt;i})\)</span> as a posterior sample to estimate <span class="math inline">\(\log P(D_i|D_{&lt;i})\)</span>
</li>
<li>
and then add this estimate to a running sum.
</li>
</ul>
<p>In the worst case, this procedure can be enormously expensive, requiring us to run gradient descent to convergence <span class="math inline">\(n\)</span> times. In practice we found that this wasn’t as expensive as it might as first seem, because if the model fits the data well then the optimal parameters for <span class="math inline">\(D_{\le i}\)</span> should be similar to those for <span class="math inline">\(D_{&lt;i}\)</span>, so it won’t take long for gradient descent to reach them.</p>
</div>
<h3 id="linear-model-combinations-and-ld">Linear model combinations and <span class="math inline">\(L(D)\)</span></h3>
<p><strong>TL;DR:</strong> if you train a linear regressor on top of predictions from Bayesian models concurrently as the models are being updated, then the model with the highest weight is the one with the highest <span class="math inline">\(L(D)\)</span>, not the one whose final posterior is the best fit.</p>
<button onclick="hideDiv('linearcombo')">
Toggle details.
</button>
<div id="linearcombo" style="display:none;">
<p>We can use gradient descent to estimate <span class="math inline">\(L(D)\)</span>, but it also turns out that <span class="math inline">\(L(D)\)</span> can tell us something about why gradient descent might prefer models with a high (lower bound on their) marginal likelihood. We’ll focus on the setting where we train linear combinations of models while these models are themselves performing Bayesian updating, in a loose sense mimicking what happens when we optimize a hierarchical model like a neural network.</p>
<p>We’ll assume some collection of Bayesian linear regression models <span class="math inline">\(M_1, \dots, M_k\)</span> and a regression data set of the form <span class="math inline">\(D = (X_i, Y_i)_{i=1}^N\)</span>; each of these models <span class="math inline">\(M_j\)</span> will output predictions <span class="math inline">\(\hat{Y^j_i}\)</span> sampled from its posterior <span class="math inline">\(P(Y_i|D_{&lt;i}, X_i)\)</span>. On top of these models we are training a linear predictor <span class="math inline">\(w\)</span> to minimize the objective <span class="math inline">\(\|w^\top(\widehat{Y}_i) - Y_i\|^2\)</span>. We’ll assume that these models make predictions of a similar scale (i.e. there isn’t a model that consistently outputs <span class="math inline">\(0.0000001 \times Y_i\)</span> whose optimal weight is then 10000000, while another model which consistently outputs <span class="math inline">\(Y_i\)</span> would get weight 1), and that they don’t have complementary errors (i.e. it’s not the case that model 1 consistently underestimates the target <span class="math inline">\(Y\)</span> while model 2 consistently overestimates it in such a way that the optimal solution is to assign a high weight to both so that they cancel each other out). Under these two assumptions, we can show that under the following optimization problem</p>
<p><span class="math display">\[ w^* = \text{arg min}_w  \sum \mathbb{E}_{\hat{Y}_i \sim P(\cdot | D_{&lt;i})} \bigg [  \|w^\top \widehat{Y}_i - Y_i\|^2 \bigg ]\]</span></p>
<p>the optimal solution <span class="math inline">\(w^*\)</span> will assign the highest weight to the model with the highest <span class="math inline">\(\mathcal{L}(D)\)</span>. In other words, running this concurrent optimization procedure and then doing magnitude pruning on the linear combination is going to be equivalent (in expectation) to picking the model with the highest lower bound on its marginal likelihood.</p>
</div>
<h3 id="empirical-results">Empirical Results</h3>
<p><strong>TL;DR:</strong> our estimators agree with the marginal likelihood on what the ‘best’ model is in a number of different settings.</p>
<div class="figure" style="width:90%; margin-bottom: 20px; margin-right:auto; margin-left:auto;">
<img width="30%" src="../images/estimator_comparison_dims.pdf" /><img width="30%" src="../images/feature_selection.png" /> <img width="30%" src="../images/prior_variance_selection.png" /> <br /> Estimated log P(D) and weight assigned by a linear model combination for various model selection problems. Our estimators give models similar rankings to the log marginal likelihood.
</div>
<button onclick="hideDiv('exp-details')">
Toggle experiment details.
</button>
<div id="exp-details" style="display:none;">
<p>We compared all three estimators on a feature selection task and found that they provided a similar model ranking to that given by the log ML. This is a nice existence proof that the lower bounds give sensible model rankings, and we provide a few more example problems in the paper. The results shown here are for the task of <em>feature selection</em> – i.e., how many (and which) features should we use in our model? We simulate a dataset inspired by  of the form <span class="math inline">\((\textbf{X}, \textbf{y})\)</span>, where <span class="math inline">\(x_i = (y_i + \epsilon_1, y_i + \dots, y_i + \epsilon_{15}, \epsilon_{16}, \dots, \epsilon_{30})\)</span>, and consider a set of models <span class="math inline">\(\{\mathcal{M}_k\}\)</span> with feature embeddings <span class="math inline">\(\phi_k(x_i) = x_i[1, \dots, k]\)</span>. The optimal model selects the first 15 features. This example is typical of a range of model selection tasks that we looked at where the hyperparameter formed a nice “Occam’s Hill”. The estimators tended to agree around the optimal hyperparameter/model, with the estimator for <span class="math inline">\(L(D)\)</span> penalizing models whose posteriors were too peaked more than the log ML did in models that didn’t fit the data well.</p>
<p>We find similar results when we compare the weight assigned to a Bayesian model in a linear model combination trained online as the Bayesian model is being iteratively updated, as in the procedure described in the previous section. We compare against two other linear model combination approaches: one where we train the linear model combination on samples from the already-fitted posterior <span class="math inline">\(P(D_i|D)\)</span>, and another where we fit the linear model combination on samples from the prior <span class="math inline">\(P(D_i|\emptyset)\)</span>. We again see that <span class="math inline">\(L(D)\)</span> and <span class="math inline">\(\log P(D)\)</span> agree close to the optimum. We also see a similar ranking of models between <span class="math inline">\(L(D)\)</span> and the linear model combination weights given by concurrent sampling – the ‘posterior sampling’ and ‘prior sampling’ approaches do not exhibit this trend, as expected.</p>
</div>
<h1 id="connection-to-deep-learning">Connection to Deep Learning</h1>
<p>Recall that the ML measures how well updates from one subset of the data generalize to unseen data points. This gives us an analogy between training speed, as measured by the sum of log predictive posterior likelihoods, and Bayesian model selection. Training speed in a Bayesian updating procedure is defined with respect to the number of data points needed to assign high likelihood to the rest of the training data. However, in many settings where we might want to do model selection, we don’t necessarily have exact posterior samples from a Bayesian model conditioned on increasing subsets of the data. Standard gradient-based optimization of deep neural networks (DNNs) involves iterating through the entire dataset repeatedly for several epochs, so we’ll need to consider training speed with respect to the number of gradient steps taken, rather than the number of data points seen. In this section, we’re going to take a first stab at the following question:</p>
<p><em>Does there exist a connection between a notion of training speed that captures within-training-set generalization in DNNs, and their test set generalization performance?</em></p>
<p>To apply the Bayesian intuition to neural networks requires us to map some of the concepts discussed earlier in the Bayesian setting to the risk minimization setting.</p>
<ul>
<li>Bayesian model <span class="math inline">\(\equiv\)</span> function approximator</li>
<li>Posterior <span class="math inline">\(P(\theta|D) \equiv\)</span> Point estimate <span class="math inline">\(\theta^*\)</span></li>
<li><span class="math inline">\(\sum \log P(D_i|D_{&lt;i}) \equiv \sum_{t=1}^T \ell(D_{i_t}, \theta_t)\)</span></li>
<li>Marginal likelihood <span class="math inline">\(\equiv\)</span> generalization error</li>
</ul>
<!-- First, we map the idea of a Bayesian model, which computes a probability distribution, to a function approximator, which provides a point estimate of a classifier. Rather than considering a Bayesian updating procedure, we’re interested in models trained with minibatch SGD to minimize some cost function. Instead of summing over predictive posterior likelihoods to estimate the log marginal likelihood, we will be summing over minibatch training losses. Finally, our metric of which model is `best’ won’t be the marginal likelihood -- instead we’re interested in the model with the lowest test set error. -->
<p>While there are many interesting lines of work framing stochastic gradient optimization as performing posterior sampling, in the analysis that follows we won’t be using this interpretation. Nor will we try to cast classifiers as computing unnormalized probability distributions. Instead, we’re going to focus on using minibatch SGD to measure how well model updates based on one subset of the data generalize to other subsets.</p>
<p>###Motivation: Minibatch Loss as a Risk Estimator
<strong>TL;DR:</strong> when you compute a minibatch gradient update, your loss on the next, disjoint minibatch gives you an idea of how well that gradient update will generalize to your test set.</p>
<button onclick="hideDiv('minibatch')">
Toggle details.
</button>
<div id="minibatch" style="display:none;">
<p>How quickly a model’s training loss decreases while training depends on a number of factors including the optimizer, hyperparameters, model architecture, and dataset. However, all else being equal, a model’s loss will decrease faster if the gradient update based on minibatch <span class="math inline">\(i\)</span> of the dataset also decreases the loss on minibatch <span class="math inline">\(i+1\)</span>. This relates to the notion of <a href="https://arxiv.org/abs/1901.09491">stiffness discussed by Fort et al.</a>, who argue that models for which gradients from minibatches all look similar should generalize better than those whose minibatch updates are orthogonal. Summing over the minibatch training losses then gives us a straightforward way to measure how well the gradients are generalizing. Models which attain a low loss with fewer training steps will have a lower sum over training losses (SOTL).</p>
<p>To illustrate how SOTL might measure within-minibatch gradient generalization, consider a first-order Taylor approximation of the training loss. The change in loss for minibatch <span class="math inline">\(i\)</span> after taking a gradient step computed on that minibatch looks as follows</p>
<p><span class="math display">\[\begin{align} \ell(D_i ; \theta_{t+1}) - \alpha g_{t}) &amp;\approx \ell(D_i; \theta_{t}) - \alpha \nabla_\theta \ell (D_i; \theta_{t}) ^\top g_{t}\\
&amp; = \ell(D_i; \theta_{t_i}) - \alpha \|g_{t}\|^2 
\end{align}\]</span></p>
<p>Meanwhile, the change due to gradients tep <span class="math inline">\(g_{t_i}\)</span> on a disjoint minibatch <span class="math inline">\(D_j\)</span> can be written
<span class="math display">\[\begin{align}
\ell(D_j; \theta_{t+1}) = \ell(D_j; \theta_t - \alpha g_{t}) &amp;\approx  \ell(D_j; \theta_t) - \nabla_\theta \ell (D_j; \theta)^\top g_t \\
&amp;=  \ell(D_j; \theta_t) - \nabla_\theta \ell (D_j; \theta)^\top \nabla_\theta \ell(D_i; \theta)
\end{align}\]</span></p>
<p>This dot product term measures how correlated gradients based on disjoint minibatches are. For the first epoch of training, this gives an unbiased estimate of the improvement to the expected risk obtained by the gradient step. After the first epoch, because the parameters will depend on all data points in the training dataset, we can’t get nice guarantees on this being a good estimate of the change in the test set loss. Nonetheless, for a large training set we can argue that because the parameters depend minimally on any particular minibatch, we should still get a pretty good estimate of the change in the test set error.</p>
<p>Concretely, we hypothesize that under fixed-stepsize SGD, models that obtain a lower sum over training losses should get the best test set performance. We evaluated a handful of different neural network architectures trained on image datasets and found that this correlation was fairly consistent. A more in-depth empirical analysis in the setting of neural architecture serach was performed by <a href="https://arxiv.org/abs/2006.04492">Ru et al</a>.</p>
</div>
<h3 id="empirical-results-1">Empirical Results</h3>
<div class="figure" style="width:40%; margin:2px; float:right;">
<img width="90%" src="../images/SOTLvsXEParallel.png" /> <br /> <br />
Models which train faster (as measured by their sum over training losses) generalize better.
</div>
We now ask whether the connection between <span class="math inline">\(\mathcal{L}(\mathcal{D})\)</span> and the weight assigned to a model in a linear model combination might also apply in some form to the deep learning setting. The lemma doing the heavy lifting in the proof of this result basically says that the weight assigned to a feature in a linear regression problem depending to some extent on that feature being fairly predictive of the target (modulo a bunch of assumptions). We make a similar conjecture for the weight assigned to activations in the penultimate layer of a neural network: features which are more predictive of the targets will be assigned higher weight. Because the final-layer weights and activations are trained concurrently, we expect that the notion of how predictive a node is will be more closely aligned with the SOTL definition rather than its final ‘training loss’.
<div class="figure" style="width:100%; margin:2px; padding:2px; float:left;">
<img align="center" width="90%" src="../images/submodel_perf.png" /> <br /> <br />Layer activations which train faster (as measured by their sum over ‘information losses’) are assigned higher weight by the final layer of a neural network.
</div>
<p>To investigate this claim, we train a linear combination of models, and see whether the test loss correlates with the assigned model weight. We observe that SGD tends to upweight models that generalize better. This suggests that SGD may implicitly perform model selection. Interestingly, we see that this phenomenon also holds for subnetworks within a network. The above figure shows that SGD upweights sub-networks with lower SOTL. These results are far from the final word on the topic, and we believe they present an interesting direction for future work.</p>
<h3 id="recap">Recap</h3>
<p>The goal of this paper was to analyze the connection between Bayesian model selection and training speed in order to gain insight into how models generalize. We proposed a family of estimators of the marginal likelihood that measure a notion of training speed, and which depend only on samples from the posteriors of Bayesian models. We further showed that this quantity is equivalent to measuring how well updates based on one subset of the data generalize to other subsets, and that an analogous measure on deep neural networks also appears to empirically correlate with generalization error. Our results thus point both to a way of doing principled Bayesian model selection on families of models from which we can obtain posterior samples, and also to a direction of inquiry towards understanding generalization in complex function approximators based on optimization trajectories rather than only post-training quantities.</p>
</div></div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="http://twitter.com/clarelyle">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://linkedin.com/in/clarelyle">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://github.com/clareification">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>

</body>

</html>
